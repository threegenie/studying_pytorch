{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04.로지스틱 회귀.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPtt5piDWuOvG6mQpH/YMzB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/threegenie/studying_pytorch/blob/main/04_%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1_%ED%9A%8C%EA%B7%80.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQtlIafURwey"
      },
      "source": [
        "> PyTorch로 시작하는 딥러닝 입문 - <nn.Module로 구현하는 로지스틱 회귀, 클래스로 파이토치 모델 구현하기>를 참고하여 직접 logistic regression model을 만들어보기(sklearn과 pytorch 비교)\n",
        "- Kaggle Titanic 데이터를 사용하여 생존자 예측(https://www.kaggle.com/c/titanic/data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT-ndyLlRrDw"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, sys\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7Z_cYkkZduP",
        "outputId": "fd165de5-711c-44c6-9e25-ad4626dbdfd1"
      },
      "source": [
        "# random seed 고정\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f22398f8b90>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUSRG5bRW6kX"
      },
      "source": [
        "### Data import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6x7T_N3SKh8"
      },
      "source": [
        "train = pd.read_csv('https://ds-lecture-data.s3.ap-northeast-2.amazonaws.com/titanic/train.csv')\n",
        "test = pd.read_csv('https://ds-lecture-data.s3.ap-northeast-2.amazonaws.com/titanic/test.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "MvSa7MW0W98m",
        "outputId": "2664e69f-ad6a-430a-e185-73a07be4f25f"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Braund, Mr. Owen Harris</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>A/5 21171</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>PC 17599</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C85</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Heikkinen, Miss. Laina</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>STON/O2. 3101282</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113803</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>C123</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Allen, Mr. William Henry</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>373450</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PassengerId  Survived  Pclass  ...     Fare Cabin  Embarked\n",
              "0            1         0       3  ...   7.2500   NaN         S\n",
              "1            2         1       1  ...  71.2833   C85         C\n",
              "2            3         1       3  ...   7.9250   NaN         S\n",
              "3            4         1       1  ...  53.1000  C123         S\n",
              "4            5         0       3  ...   8.0500   NaN         S\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20XhzmmTXAj8"
      },
      "source": [
        "### Scikit-Learn으로 Logistic Regression 모델 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhSXIUFVWtFd"
      },
      "source": [
        "# train, val split\n",
        "from sklearn.model_selection import train_test_split\n",
        "train, val = train_test_split(train, random_state=2)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7jnlw5bXLhq"
      },
      "source": [
        "target = 'Survived'\n",
        "features = ['Pclass','SibSp','Parch','Fare'] #scaling하기 귀찮아서 numeric한 특성만 사용.."
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLS1eGbQXNVK"
      },
      "source": [
        "y_train = train[target]\n",
        "x_train = train[features]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HJ7G71dYz1J"
      },
      "source": [
        "x_val = val[features]\n",
        "y_val = val[target]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOCypo5hXQIL",
        "outputId": "1973c789-284b-49b2-f146-4de52d316f68"
      },
      "source": [
        "# logistic regression model import\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logistic = LogisticRegression()\n",
        "logistic.fit(x_train, y_train)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVfQ_rlvY8mI",
        "outputId": "de01de15-ad72-4741-d94b-b02fdf09e0bc"
      },
      "source": [
        "print('검증세트 정확도', logistic.score(x_val, y_val))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "검증세트 정확도 0.6860986547085202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TV0fDH2-YMqm",
        "outputId": "c085b020-a147-4a17-a63e-a37eff7bf723"
      },
      "source": [
        "pred = logistic.predict(x_val)\n",
        "pred"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
              "       1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,\n",
              "       0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFmqsiRFZJvq"
      },
      "source": [
        "### Pytorch로 Logistic Regression 모델 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqBEKReAcoGk"
      },
      "source": [
        "#1차원으로 만들기 위해 타겟 원소 따로 빼서 numpy array로 만듦\n",
        "y_train = [[x] for x in train['Survived']]\n",
        "y_val = [[x] for x in val['Survived']]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfBJSH9MdUGX"
      },
      "source": [
        "# 타겟 numpy to tensor\n",
        "y_train_ts = torch.Tensor(y_train)\n",
        "y_val_ts = torch.Tensor(y_val)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRz0_AERbLS5"
      },
      "source": [
        "# convert pandas dataframe to numpy array\n",
        "trainX = torch.tensor(x_train.values)\n",
        "valX = torch.tensor(x_val.values)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-Je3dUvZ5Hl"
      },
      "source": [
        "# numpy array to Torch tensor\n",
        "X_train_ts = trainX.float()\n",
        "X_val_ts = valX.float()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--W25AKgZ9Rm"
      },
      "source": [
        "# long tensor to float tensor\n",
        "X_train_ts = X_train_ts.float()\n",
        "y_train_ts = y_train_ts.float()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qap__d3xaAz0"
      },
      "source": [
        "# logistic regression model built\n",
        "class BinaryClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(4, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.sigmoid(self.linear(x))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mno-yOjnaIVz"
      },
      "source": [
        "model = BinaryClassifier()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFeEArWVaJuZ",
        "outputId": "ff325353-25ae-47b7-b6f3-78ad33dd2744"
      },
      "source": [
        "# optimizer 설정\n",
        "optimizer = optim.SGD(model.parameters(), lr=1)\n",
        "\n",
        "nb_epochs = 2000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "    # H(x) 계산\n",
        "    hypothesis = model(X_train_ts)\n",
        "\n",
        "    # cost 계산\n",
        "    cost = F.binary_cross_entropy(hypothesis, y_train_ts)\n",
        "\n",
        "    # cost로 H(x) 개선\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # 20번마다 로그 출력\n",
        "    if epoch % 10 == 0:\n",
        "        prediction = hypothesis >= torch.FloatTensor([0.5]) # 예측값이 0.5를 넘으면 True로 간주\n",
        "        correct_prediction = prediction.float() == y_train_ts # 실제값과 일치하는 경우만 True로 간주\n",
        "        accuracy = correct_prediction.sum().item() / len(correct_prediction) # 정확도를 계산\n",
        "        print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format( # 각 에포크마다 정확도를 출력\n",
        "            epoch, nb_epochs, cost.item(), accuracy * 100,\n",
        "        ))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0/2000 Cost: 35.536613 Accuracy 62.57%\n",
            "Epoch   10/2000 Cost: 35.536331 Accuracy 62.57%\n",
            "Epoch   20/2000 Cost: 35.536057 Accuracy 62.57%\n",
            "Epoch   30/2000 Cost: 35.535782 Accuracy 62.57%\n",
            "Epoch   40/2000 Cost: 35.535500 Accuracy 62.57%\n",
            "Epoch   50/2000 Cost: 35.535233 Accuracy 62.57%\n",
            "Epoch   60/2000 Cost: 35.534958 Accuracy 62.57%\n",
            "Epoch   70/2000 Cost: 35.534691 Accuracy 62.57%\n",
            "Epoch   80/2000 Cost: 35.534420 Accuracy 62.57%\n",
            "Epoch   90/2000 Cost: 35.534157 Accuracy 62.57%\n",
            "Epoch  100/2000 Cost: 35.533886 Accuracy 62.57%\n",
            "Epoch  110/2000 Cost: 35.533623 Accuracy 62.57%\n",
            "Epoch  120/2000 Cost: 35.533363 Accuracy 62.57%\n",
            "Epoch  130/2000 Cost: 35.533100 Accuracy 62.57%\n",
            "Epoch  140/2000 Cost: 35.532841 Accuracy 62.57%\n",
            "Epoch  150/2000 Cost: 35.532581 Accuracy 62.57%\n",
            "Epoch  160/2000 Cost: 35.532322 Accuracy 62.57%\n",
            "Epoch  170/2000 Cost: 35.532066 Accuracy 62.57%\n",
            "Epoch  180/2000 Cost: 35.531818 Accuracy 62.57%\n",
            "Epoch  190/2000 Cost: 35.531563 Accuracy 62.57%\n",
            "Epoch  200/2000 Cost: 35.531307 Accuracy 62.57%\n",
            "Epoch  210/2000 Cost: 35.531055 Accuracy 62.57%\n",
            "Epoch  220/2000 Cost: 35.530804 Accuracy 62.57%\n",
            "Epoch  230/2000 Cost: 35.530560 Accuracy 62.57%\n",
            "Epoch  240/2000 Cost: 35.530308 Accuracy 62.57%\n",
            "Epoch  250/2000 Cost: 35.530064 Accuracy 62.57%\n",
            "Epoch  260/2000 Cost: 35.529816 Accuracy 62.57%\n",
            "Epoch  270/2000 Cost: 35.529572 Accuracy 62.57%\n",
            "Epoch  280/2000 Cost: 35.529331 Accuracy 62.57%\n",
            "Epoch  290/2000 Cost: 35.529091 Accuracy 62.57%\n",
            "Epoch  300/2000 Cost: 35.528847 Accuracy 62.57%\n",
            "Epoch  310/2000 Cost: 35.528606 Accuracy 62.57%\n",
            "Epoch  320/2000 Cost: 35.528374 Accuracy 62.57%\n",
            "Epoch  330/2000 Cost: 35.528133 Accuracy 62.57%\n",
            "Epoch  340/2000 Cost: 35.527897 Accuracy 62.57%\n",
            "Epoch  350/2000 Cost: 35.527664 Accuracy 62.57%\n",
            "Epoch  360/2000 Cost: 35.527424 Accuracy 62.57%\n",
            "Epoch  370/2000 Cost: 35.527195 Accuracy 62.57%\n",
            "Epoch  380/2000 Cost: 35.526962 Accuracy 62.57%\n",
            "Epoch  390/2000 Cost: 35.526730 Accuracy 62.57%\n",
            "Epoch  400/2000 Cost: 35.526501 Accuracy 62.57%\n",
            "Epoch  410/2000 Cost: 35.526272 Accuracy 62.57%\n",
            "Epoch  420/2000 Cost: 35.526047 Accuracy 62.57%\n",
            "Epoch  430/2000 Cost: 35.525818 Accuracy 62.57%\n",
            "Epoch  440/2000 Cost: 35.525593 Accuracy 62.57%\n",
            "Epoch  450/2000 Cost: 35.525364 Accuracy 62.57%\n",
            "Epoch  460/2000 Cost: 35.525143 Accuracy 62.57%\n",
            "Epoch  470/2000 Cost: 35.524921 Accuracy 62.57%\n",
            "Epoch  480/2000 Cost: 35.524696 Accuracy 62.57%\n",
            "Epoch  490/2000 Cost: 35.524475 Accuracy 62.57%\n",
            "Epoch  500/2000 Cost: 35.507370 Accuracy 62.57%\n",
            "Epoch  510/2000 Cost: 35.507149 Accuracy 62.57%\n",
            "Epoch  520/2000 Cost: 35.506931 Accuracy 62.57%\n",
            "Epoch  530/2000 Cost: 35.506706 Accuracy 62.57%\n",
            "Epoch  540/2000 Cost: 35.506481 Accuracy 62.57%\n",
            "Epoch  550/2000 Cost: 35.506264 Accuracy 62.57%\n",
            "Epoch  560/2000 Cost: 35.506042 Accuracy 62.57%\n",
            "Epoch  570/2000 Cost: 35.505821 Accuracy 62.57%\n",
            "Epoch  580/2000 Cost: 35.505611 Accuracy 62.57%\n",
            "Epoch  590/2000 Cost: 35.505390 Accuracy 62.57%\n",
            "Epoch  600/2000 Cost: 35.505177 Accuracy 62.57%\n",
            "Epoch  610/2000 Cost: 35.504959 Accuracy 62.57%\n",
            "Epoch  620/2000 Cost: 35.504749 Accuracy 62.57%\n",
            "Epoch  630/2000 Cost: 35.504539 Accuracy 62.57%\n",
            "Epoch  640/2000 Cost: 35.504326 Accuracy 62.57%\n",
            "Epoch  650/2000 Cost: 35.504116 Accuracy 62.57%\n",
            "Epoch  660/2000 Cost: 35.503906 Accuracy 62.57%\n",
            "Epoch  670/2000 Cost: 35.503696 Accuracy 62.57%\n",
            "Epoch  680/2000 Cost: 35.503487 Accuracy 62.57%\n",
            "Epoch  690/2000 Cost: 35.503281 Accuracy 62.57%\n",
            "Epoch  700/2000 Cost: 35.503075 Accuracy 62.57%\n",
            "Epoch  710/2000 Cost: 35.502872 Accuracy 62.57%\n",
            "Epoch  720/2000 Cost: 35.502666 Accuracy 62.57%\n",
            "Epoch  730/2000 Cost: 35.502460 Accuracy 62.57%\n",
            "Epoch  740/2000 Cost: 35.502258 Accuracy 62.57%\n",
            "Epoch  750/2000 Cost: 35.502060 Accuracy 62.57%\n",
            "Epoch  760/2000 Cost: 35.501854 Accuracy 62.57%\n",
            "Epoch  770/2000 Cost: 35.501656 Accuracy 62.57%\n",
            "Epoch  780/2000 Cost: 35.501457 Accuracy 62.57%\n",
            "Epoch  790/2000 Cost: 35.501255 Accuracy 62.57%\n",
            "Epoch  800/2000 Cost: 35.501060 Accuracy 62.57%\n",
            "Epoch  810/2000 Cost: 35.500862 Accuracy 62.57%\n",
            "Epoch  820/2000 Cost: 35.500668 Accuracy 62.57%\n",
            "Epoch  830/2000 Cost: 35.500473 Accuracy 62.57%\n",
            "Epoch  840/2000 Cost: 35.500271 Accuracy 62.57%\n",
            "Epoch  850/2000 Cost: 35.500080 Accuracy 62.57%\n",
            "Epoch  860/2000 Cost: 35.499889 Accuracy 62.57%\n",
            "Epoch  870/2000 Cost: 35.499695 Accuracy 62.57%\n",
            "Epoch  880/2000 Cost: 35.499504 Accuracy 62.57%\n",
            "Epoch  890/2000 Cost: 35.499313 Accuracy 62.57%\n",
            "Epoch  900/2000 Cost: 35.499123 Accuracy 62.57%\n",
            "Epoch  910/2000 Cost: 35.498936 Accuracy 62.57%\n",
            "Epoch  920/2000 Cost: 35.498745 Accuracy 62.57%\n",
            "Epoch  930/2000 Cost: 35.498558 Accuracy 62.57%\n",
            "Epoch  940/2000 Cost: 35.498371 Accuracy 62.57%\n",
            "Epoch  950/2000 Cost: 35.498184 Accuracy 62.57%\n",
            "Epoch  960/2000 Cost: 35.498005 Accuracy 62.57%\n",
            "Epoch  970/2000 Cost: 35.497814 Accuracy 62.57%\n",
            "Epoch  980/2000 Cost: 35.497635 Accuracy 62.57%\n",
            "Epoch  990/2000 Cost: 35.497452 Accuracy 62.57%\n",
            "Epoch 1000/2000 Cost: 35.497269 Accuracy 62.57%\n",
            "Epoch 1010/2000 Cost: 35.497082 Accuracy 62.57%\n",
            "Epoch 1020/2000 Cost: 35.496910 Accuracy 62.57%\n",
            "Epoch 1030/2000 Cost: 35.496727 Accuracy 62.57%\n",
            "Epoch 1040/2000 Cost: 35.496544 Accuracy 62.57%\n",
            "Epoch 1050/2000 Cost: 35.496368 Accuracy 62.57%\n",
            "Epoch 1060/2000 Cost: 35.496189 Accuracy 62.57%\n",
            "Epoch 1070/2000 Cost: 35.496014 Accuracy 62.57%\n",
            "Epoch 1080/2000 Cost: 35.495834 Accuracy 62.57%\n",
            "Epoch 1090/2000 Cost: 35.495663 Accuracy 62.57%\n",
            "Epoch 1100/2000 Cost: 35.495487 Accuracy 62.57%\n",
            "Epoch 1110/2000 Cost: 35.495312 Accuracy 62.57%\n",
            "Epoch 1120/2000 Cost: 35.495136 Accuracy 62.57%\n",
            "Epoch 1130/2000 Cost: 35.494961 Accuracy 62.57%\n",
            "Epoch 1140/2000 Cost: 35.494789 Accuracy 62.57%\n",
            "Epoch 1150/2000 Cost: 35.494621 Accuracy 62.57%\n",
            "Epoch 1160/2000 Cost: 35.494446 Accuracy 62.57%\n",
            "Epoch 1170/2000 Cost: 35.494270 Accuracy 62.57%\n",
            "Epoch 1180/2000 Cost: 35.494102 Accuracy 62.57%\n",
            "Epoch 1190/2000 Cost: 35.493935 Accuracy 62.57%\n",
            "Epoch 1200/2000 Cost: 35.493767 Accuracy 62.57%\n",
            "Epoch 1210/2000 Cost: 35.493599 Accuracy 62.57%\n",
            "Epoch 1220/2000 Cost: 35.493435 Accuracy 62.57%\n",
            "Epoch 1230/2000 Cost: 35.493267 Accuracy 62.57%\n",
            "Epoch 1240/2000 Cost: 35.493099 Accuracy 62.57%\n",
            "Epoch 1250/2000 Cost: 35.492935 Accuracy 62.57%\n",
            "Epoch 1260/2000 Cost: 35.492771 Accuracy 62.57%\n",
            "Epoch 1270/2000 Cost: 35.492607 Accuracy 62.57%\n",
            "Epoch 1280/2000 Cost: 35.492443 Accuracy 62.57%\n",
            "Epoch 1290/2000 Cost: 35.492283 Accuracy 62.57%\n",
            "Epoch 1300/2000 Cost: 35.492119 Accuracy 62.57%\n",
            "Epoch 1310/2000 Cost: 35.491955 Accuracy 62.57%\n",
            "Epoch 1320/2000 Cost: 35.491795 Accuracy 62.57%\n",
            "Epoch 1330/2000 Cost: 35.491631 Accuracy 62.57%\n",
            "Epoch 1340/2000 Cost: 35.491474 Accuracy 62.57%\n",
            "Epoch 1350/2000 Cost: 35.491318 Accuracy 62.57%\n",
            "Epoch 1360/2000 Cost: 35.491158 Accuracy 62.57%\n",
            "Epoch 1370/2000 Cost: 35.491001 Accuracy 62.57%\n",
            "Epoch 1380/2000 Cost: 35.490841 Accuracy 62.57%\n",
            "Epoch 1390/2000 Cost: 35.490688 Accuracy 62.57%\n",
            "Epoch 1400/2000 Cost: 35.490528 Accuracy 62.57%\n",
            "Epoch 1410/2000 Cost: 35.490368 Accuracy 62.57%\n",
            "Epoch 1420/2000 Cost: 35.490215 Accuracy 62.57%\n",
            "Epoch 1430/2000 Cost: 35.490067 Accuracy 62.57%\n",
            "Epoch 1440/2000 Cost: 35.489910 Accuracy 62.57%\n",
            "Epoch 1450/2000 Cost: 35.489761 Accuracy 62.57%\n",
            "Epoch 1460/2000 Cost: 35.489601 Accuracy 62.57%\n",
            "Epoch 1470/2000 Cost: 35.489452 Accuracy 62.57%\n",
            "Epoch 1480/2000 Cost: 35.489300 Accuracy 62.57%\n",
            "Epoch 1490/2000 Cost: 35.489143 Accuracy 62.57%\n",
            "Epoch 1500/2000 Cost: 35.488995 Accuracy 62.57%\n",
            "Epoch 1510/2000 Cost: 35.488846 Accuracy 62.57%\n",
            "Epoch 1520/2000 Cost: 35.488697 Accuracy 62.57%\n",
            "Epoch 1530/2000 Cost: 35.488552 Accuracy 62.57%\n",
            "Epoch 1540/2000 Cost: 35.488400 Accuracy 62.57%\n",
            "Epoch 1550/2000 Cost: 35.488251 Accuracy 62.57%\n",
            "Epoch 1560/2000 Cost: 35.488106 Accuracy 62.57%\n",
            "Epoch 1570/2000 Cost: 35.487961 Accuracy 62.57%\n",
            "Epoch 1580/2000 Cost: 35.487812 Accuracy 62.57%\n",
            "Epoch 1590/2000 Cost: 35.487667 Accuracy 62.57%\n",
            "Epoch 1600/2000 Cost: 35.487522 Accuracy 62.57%\n",
            "Epoch 1610/2000 Cost: 35.487373 Accuracy 62.57%\n",
            "Epoch 1620/2000 Cost: 35.487236 Accuracy 62.57%\n",
            "Epoch 1630/2000 Cost: 35.487087 Accuracy 62.57%\n",
            "Epoch 1640/2000 Cost: 35.486942 Accuracy 62.57%\n",
            "Epoch 1650/2000 Cost: 35.486805 Accuracy 62.57%\n",
            "Epoch 1660/2000 Cost: 35.486660 Accuracy 62.57%\n",
            "Epoch 1670/2000 Cost: 35.486519 Accuracy 62.57%\n",
            "Epoch 1680/2000 Cost: 35.486382 Accuracy 62.57%\n",
            "Epoch 1690/2000 Cost: 35.486233 Accuracy 62.57%\n",
            "Epoch 1700/2000 Cost: 35.486095 Accuracy 62.57%\n",
            "Epoch 1710/2000 Cost: 35.485958 Accuracy 62.57%\n",
            "Epoch 1720/2000 Cost: 35.485821 Accuracy 62.57%\n",
            "Epoch 1730/2000 Cost: 35.485680 Accuracy 62.57%\n",
            "Epoch 1740/2000 Cost: 35.485546 Accuracy 62.57%\n",
            "Epoch 1750/2000 Cost: 35.485405 Accuracy 62.57%\n",
            "Epoch 1760/2000 Cost: 35.485271 Accuracy 62.57%\n",
            "Epoch 1770/2000 Cost: 35.485130 Accuracy 62.57%\n",
            "Epoch 1780/2000 Cost: 35.484993 Accuracy 62.57%\n",
            "Epoch 1790/2000 Cost: 35.484856 Accuracy 62.57%\n",
            "Epoch 1800/2000 Cost: 35.484718 Accuracy 62.57%\n",
            "Epoch 1810/2000 Cost: 35.484589 Accuracy 62.57%\n",
            "Epoch 1820/2000 Cost: 35.484455 Accuracy 62.57%\n",
            "Epoch 1830/2000 Cost: 35.484318 Accuracy 62.57%\n",
            "Epoch 1840/2000 Cost: 35.484184 Accuracy 62.57%\n",
            "Epoch 1850/2000 Cost: 35.484051 Accuracy 62.57%\n",
            "Epoch 1860/2000 Cost: 35.483921 Accuracy 62.57%\n",
            "Epoch 1870/2000 Cost: 35.483788 Accuracy 62.57%\n",
            "Epoch 1880/2000 Cost: 35.483654 Accuracy 62.57%\n",
            "Epoch 1890/2000 Cost: 35.483528 Accuracy 62.57%\n",
            "Epoch 1900/2000 Cost: 35.483398 Accuracy 62.57%\n",
            "Epoch 1910/2000 Cost: 35.483265 Accuracy 62.57%\n",
            "Epoch 1920/2000 Cost: 35.483131 Accuracy 62.57%\n",
            "Epoch 1930/2000 Cost: 35.483002 Accuracy 62.57%\n",
            "Epoch 1940/2000 Cost: 35.482880 Accuracy 62.57%\n",
            "Epoch 1950/2000 Cost: 35.482750 Accuracy 62.57%\n",
            "Epoch 1960/2000 Cost: 35.482620 Accuracy 62.57%\n",
            "Epoch 1970/2000 Cost: 35.482491 Accuracy 62.57%\n",
            "Epoch 1980/2000 Cost: 35.482365 Accuracy 62.57%\n",
            "Epoch 1990/2000 Cost: 35.482239 Accuracy 62.57%\n",
            "Epoch 2000/2000 Cost: 35.482113 Accuracy 62.57%\n"
          ]
        }
      ]
    }
  ]
}